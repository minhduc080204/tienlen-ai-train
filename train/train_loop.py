# train/train_loop.py

import torch

from env.tienlen_env import TienLenEnv
from state.state_encoder import encode_state
from action.action_mask import build_action_mask
from action.action_space import ACTION_SPACE

from rl.agent import PPOAgent
from rl.model import TienLenPolicy
from rl.buffer import RolloutBuffer
from env.reward import compute_reward

from core.action_executor import resolve_action
from env.step_result import StepResult

from bots.rule_bot import RuleBot
from state.state_dim import STATE_DIM
import argparse
import train.config as config
from utils.logger import setup_logger

def parse_args():
    parser = argparse.ArgumentParser("PPO Train Ti·∫øn L√™n")

    parser.add_argument("--episodes", type=int, default=config.MAX_EPISODES)
    parser.add_argument("--batch-size", type=int, default=config.BATCH_SIZE)
    parser.add_argument("--lr", type=float, default=config.LR)

    parser.add_argument("--gamma", type=float, default=config.GAMMA)
    parser.add_argument("--lam", type=float, default=config.LAMBDA)

    parser.add_argument("--ppo-epochs", type=int, default=config.PPO_EPOCHS)

    parser.add_argument("--save-every", type=int, default=config.SAVE_EVERY)
    parser.add_argument("--eval-every", type=int, default=config.EVAL_EVERY)

    parser.add_argument("--device", type=str, default="auto",
                        choices=["auto", "cpu", "cuda"])

    return parser.parse_args()


def train():
    logger = setup_logger(
        name="ppo_train",
        log_dir="logs"
    )

    args = parse_args()
    MAX_EPISODES = args.episodes
    BATCH_SIZE = args.batch_size
    LR = args.lr
    GAMMA = args.gamma
    LAMBDA = args.lam
    PPO_EPOCHS = args.ppo_epochs
    SAVE_EVERY=args.save_every

    NUM_PLAYERS=config.NUM_PLAYERS
    MAX_TURNS_PER_EP=config.MAX_TURNS_PER_EP
    AI_PLAYER_ID=config.AI_PLAYER_ID
    
    print("üöÄ Start PPO Training")

    # =========================
    # 1Ô∏è‚É£ INIT
    # =========================
    env = TienLenEnv(num_players=NUM_PLAYERS)

    model = TienLenPolicy(
        state_dim=STATE_DIM,
        action_dim=len(ACTION_SPACE)
    )

    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    agent = PPOAgent(
        model=model,
        optimizer=optimizer,
        gamma=GAMMA,
        clip_eps=0.2
    )


    rule_bot = RuleBot(player_id=1)
    buffer = RolloutBuffer()

    # =========================
    # 2Ô∏è‚É£ TRAIN LOOP
    # =========================
    for episode in range(1, MAX_EPISODES + 1):

        state = env.reset()

        # instant win ‚Üí skip
        if state.finished:
            continue

        buffer.clear()
        done = False
        turn_count = 0

        # =========================
        # 3Ô∏è‚É£ PLAY ONE GAME
        # =========================
        while not done:
            turn_count += 1
            current_pid = env.state.current_player
            if turn_count >= MAX_TURNS_PER_EP:
                print(f"‚ö†Ô∏è Episode {episode} force stop (turn limit)")
                break

            # =====================
            # AI TURN
            # =====================
            if current_pid == AI_PLAYER_ID:
                player_hand = state.hands[AI_PLAYER_ID]

                opponent_hands = [
                    hand
                    for pid, hand in enumerate(state.hands)
                    if pid != AI_PLAYER_ID
                ]

                # discard_pile = env.discard_pile   # ho·∫∑c [] n·∫øu ch∆∞a c√≥
                discard_pile = []   # ho·∫∑c [] n·∫øu ch∆∞a c√≥

                state_vec = encode_state(
                    hand=player_hand,
                    discard_pile=discard_pile,
                    opponent_hands=opponent_hands,
                    current_trick=state.current_trick,
                    player_id=AI_PLAYER_ID,
                    num_players=NUM_PLAYERS
                )

                state_tensor = torch.tensor(
                    state_vec,
                    dtype=torch.float32
                ).unsqueeze(0)   # (1, STATE_DIM)

                action_mask = build_action_mask(
                    hand=env.state.hands[AI_PLAYER_ID],
                    current_trick=env.state.current_trick
                )

                action_mask = torch.tensor(
                    action_mask,
                    dtype=torch.float32
                ).unsqueeze(0)   # (1, action_dim)


                action_id, logprob, value = agent.act(
                    state_tensor,
                    action_mask
                )

                action_spec = ACTION_SPACE[action_id]

                action_cards = resolve_action(
                    action_spec=action_spec,
                    hand=env.state.hands[AI_PLAYER_ID],
                    current_trick=env.state.current_trick
                )


            # =====================
            # OPPONENT TURN
            # =====================
            else:
                action_cards = rule_bot.select_action(
                    state,
                    current_pid
                )

            # =====================
            # ENV STEP
            # =====================
            step_result: StepResult = env.step(action_cards)

            reward = step_result.reward

            # =====================
            # STORE ROLLOUT (AI ONLY)
            # =====================
            if current_pid == AI_PLAYER_ID:
                buffer.add(
                    state=state_vec,
                    action=action_id,
                    logprob=logprob.detach(),
                    reward=reward,
                    done=step_result.done,
                    value=value.detach(),
                    action_mask=action_mask
                )

            state = step_result.state
            done = step_result.done

            # =========================
            # 4Ô∏è‚É£ PPO UPDATE
            # =========================
            if len(buffer) > 0:
                advantages, returns = buffer.compute_gae(
                    gamma=GAMMA,
                    lam=LAMBDA
                )

                agent.update(
                    states=buffer.states,
                    actions=buffer.actions,
                    old_logprobs=buffer.logprobs,
                    returns=returns,
                    advantages=advantages,
                    action_masks=buffer.action_masks,
                    epochs=PPO_EPOCHS,
                    batch_size=BATCH_SIZE
                )

                buffer.clear()


        # =========================
        # 5Ô∏è‚É£ LOG
        # =========================
        if episode % 1 == 0:
            win = state.winner == AI_PLAYER_ID
            print(
                f"[Episode {episode}] "
                f"Turns={turn_count} "
                f"Win={win}"
            )
            logger.info(
                f"Episode={episode} "
                f"Turns={turn_count} "
                f"Win={win} "
                f"TotalReward={reward:.3f}"
            )


        # =========================
        # 6Ô∏è‚É£ SAVE CHECKPOINT
        # =========================
        if episode % SAVE_EVERY == 0:
            path = f"checkpoints/ppo_ep{episode}.pt"
            agent.save(path)
            print(f"üíæ Saved checkpoint: {path}")

    print("‚úÖ Training finished")


if __name__ == "__main__":
    train()
